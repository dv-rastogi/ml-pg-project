{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch.utils.data as Data\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_FEATURES = 82\n",
    "DAYS = 60\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockMarketDataReg(Data.Dataset):\n",
    "    def __init__(self, train=True, split=0.2, target_market=\"S&P\", days=60):\n",
    "        self.days = days\n",
    "        df = pd.read_csv(f\"../data/Processed_{target_market}.csv\")\n",
    "        df['Date'] = pd.to_datetime(df['Date'], format='%Y-%m-%d')\n",
    "        df = df.sort_values(by='Date')\n",
    "        df = df.drop(columns=['Name', 'Date'])\n",
    "        df = df.fillna(0)\n",
    "        # outlier detection\n",
    "        for col in df.columns:\n",
    "            q1, q3 = df[col].quantile([0.25, 0.75])\n",
    "            iqr = q3 - q1\n",
    "            lb, rb = q1 - 1.5 * iqr, q3 + 1.5 * iqr\n",
    "            for i in range(len(df)):\n",
    "                if df[col][i] > rb:\n",
    "                    df[col][i] = rb\n",
    "                if df[col][i] < lb:\n",
    "                    df[col][i] = lb\n",
    "        self.features = df.shape[1]\n",
    "        num_rows_init = df.shape[0]\n",
    "        u = df.iloc[0:int(num_rows_init*(1-split))]\n",
    "        self.mean = u.mean()\n",
    "        self.std = u.std()\n",
    "        if not train:\n",
    "            u = df.iloc[int(num_rows_init*(1-split)) - self.days:]\n",
    "        self.num_rows = u.shape[0]\n",
    "        u = (u - self.mean)/self.std\n",
    "        self.market_data = u\n",
    "    \n",
    "    def get_label(self, idx):\n",
    "        return torch.tensor([float(self.market_data.iloc[idx + 1][\"Close\"])])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        end_idx = self.days + idx - 1\n",
    "        lab = self.get_label(end_idx)\n",
    "        df = self.market_data\n",
    "        market_tens = torch.reshape(torch.from_numpy(np.array(df.iloc[idx:end_idx+1])), (self.features, self.days))\n",
    "        return market_tens.float(), lab\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_rows - self.days\n",
    "    \n",
    "    def var(self):\n",
    "        return self.std['Close']**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = StockMarketDataReg(train=True, target_market=\"S&P\", split=0.2, days=DAYS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.09239842726081259, 0.9076015727391874)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prev = None\n",
    "c1, c0 = 0, 0\n",
    "for _, y in train_ds:\n",
    "    if prev is None:\n",
    "        prev = y\n",
    "        continue\n",
    "    c0 += (y < prev).item()\n",
    "    c1 += (y >= prev).item()\n",
    "c0 / (c0 + c1), c1 / (c0 + c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = Data.DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, num_features, days, batch_size, hidden_size=100):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.num_features = num_features\n",
    "        self.days = days\n",
    "        self.hidden_size = hidden_size\n",
    "        # model\n",
    "        self.lstm = nn.LSTM(input_size=self.num_features, hidden_size=self.hidden_size, num_layers=1, batch_first=True)\n",
    "        self.decoder = nn.Linear(self.hidden_size*self.days, 1)\n",
    "    def forward(self, inp):\n",
    "        inp = torch.transpose(inp, 1, 2)\n",
    "        out1, _ = self.lstm(inp)\n",
    "        out1 = out1.reshape((self.batch_size, -1))\n",
    "        out2 = self.decoder(out1)\n",
    "        out2 = out2.squeeze()\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = Generator(NUM_FEATURES, DAYS, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, num_features, batch_size, c=4):\n",
    "        # c denotes flatenning constant\n",
    "        super().__init__()\n",
    "        self.num_features = num_features\n",
    "        self.batch_size = batch_size\n",
    "        self.convs = nn.Sequential(\n",
    "            nn.Conv1d(self.num_features, 32, kernel_size=5, stride=2),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Conv1d(32, 64, kernel_size=5, stride=2),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.BatchNorm1d(64, momentum=0.9, eps=1e-05), \n",
    "            nn.Conv1d(64, 128, kernel_size=5, stride=2),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.BatchNorm1d(128, momentum=0.9, eps=1e-05),\n",
    "        )\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Linear(128*c, 220),\n",
    "            nn.BatchNorm1d(220, momentum=0.9, eps=1e-05),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Linear(220, 220),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(220, 1)\n",
    "        )\n",
    "    def forward(self, inp):\n",
    "        out1 = self.convs(inp)\n",
    "        out1 = out1.reshape(self.batch_size, -1)\n",
    "        out2 = self.dense(out1)\n",
    "        out2 = out2.squeeze()\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "disc = Discriminator(NUM_FEATURES, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "adversarial_loss = torch.nn.BCELoss()\n",
    "\n",
    "generator = Generator(NUM_FEATURES, DAYS, BATCH_SIZE)\n",
    "discriminator = Discriminator(NUM_FEATURES, BATCH_SIZE)\n",
    "    \n",
    "    \n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=0.002)\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=0.002)\n",
    "\n",
    "\n",
    "def Train_Loss(generator, discrimitator, optimizer_G, optimizer_D, generated_data, real_data):\n",
    "    \n",
    "    g_loss = adversarial_loss(discriminator(generated_data), valid)\n",
    "    g_loss.backward()\n",
    "    optimizer_G.step()\n",
    "    optimizer_D.zero_grad()\n",
    "\n",
    "    # Valid and fake are ground truths \n",
    "    real_loss = adversarial_loss(discriminator(real_data), valid)\n",
    "    fake_loss = adversarial_loss(discriminator(generated_data.detach()), fake)\n",
    "    d_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "    d_loss.backward()\n",
    "    optimizer_D.step()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "pytorch_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
