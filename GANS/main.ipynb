{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch.utils.data as Data\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_FEATURES = 6\n",
    "DAYS = 60\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockMarketDataReg(Data.Dataset):\n",
    "    def __init__(self, train=None, split=0.2, target_market=\"S&P\", days=60):\n",
    "        self.days = days + 1 # as last entry is to be predicted\n",
    "\n",
    "        df = pd.read_csv(f\"../data/Processed_{target_market}.csv\")\n",
    "        df['Date'] = pd.to_datetime(df['Date'], format='%Y-%m-%d')\n",
    "        df = df.sort_values(by='Date')\n",
    "        df = df.drop(columns=['Name', 'Date'])\n",
    "        df = df.fillna(0)\n",
    "        \n",
    "        # outlier detection\n",
    "        for col in df.columns:\n",
    "            q1, q3 = df[col].quantile([0.25, 0.75])\n",
    "            iqr = q3 - q1\n",
    "            lb, rb = q1 - 1.5 * iqr, q3 + 1.5 * iqr\n",
    "            for i in range(len(df)):\n",
    "                if df[col][i] > rb:\n",
    "                    df[col][i] = rb\n",
    "                if df[col][i] < lb:\n",
    "                    df[col][i] = lb\n",
    "\n",
    "        self.feature_list = list(set(df.columns) - set(['Close']))\n",
    "        if train is None:\n",
    "            u = df.iloc[0: int(len(df) * (1 - split))]\n",
    "            # pca\n",
    "            self.pca = PCA(TOTAL_FEATURES - 1)\n",
    "            self.pca_data = self.pca.fit_transform(u[self.feature_list])\n",
    "            self.closing_prices = u['Close'].to_numpy()\n",
    "            self.data = np.hstack(( self.closing_prices.reshape((-1, 1)), self.pca_data ))\n",
    "        else:\n",
    "            u = df.iloc[int(len(df) * (1 - split)) - self.days:]\n",
    "            # pca\n",
    "            self.pca_data = train.pca.transform(u[self.feature_list])\n",
    "            self.closing_prices = u['Close'].to_numpy()\n",
    "            self.data = np.hstack(( self.closing_prices.reshape((-1, 1)), self.pca_data ))\n",
    "\n",
    "        self.num_rows = u.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        end_idx = self.days + idx - 1\n",
    "        market_tens = torch.from_numpy(self.data[idx: end_idx + 1])\n",
    "        return market_tens.float()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_rows - self.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = StockMarketDataReg(target_market='S&P', split=0.2, days=DAYS)\n",
    "test_ds = StockMarketDataReg(target_market='S&P', split=0.2, days=DAYS, train=train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = Data.DataLoader(train_ds, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 61, 6])"
      ]
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = next(iter(train_dataloader))\n",
    "sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    '''\n",
    "    Generates fake data using features of past DAYS days\n",
    "    '''\n",
    "    def __init__(self, num_features, days, hidden_size=300):\n",
    "        super().__init__()\n",
    "        self.num_features = num_features\n",
    "        self.days = days\n",
    "        self.hidden_size = hidden_size\n",
    "        # model\n",
    "        self.lstm = nn.LSTM(input_size=self.num_features, hidden_size=self.hidden_size, num_layers=1, batch_first=True)\n",
    "        self.decoder = nn.Sequential(\n",
    "                nn.Linear(self.hidden_size * self.days, self.num_features),\n",
    "                nn.Tanh()\n",
    "            )\n",
    "    def forward(self, inp):\n",
    "        out1, _ = self.lstm(inp)\n",
    "        out1 = out1.reshape((-1, self.hidden_size * self.days))\n",
    "        out2 = self.decoder(out1)\n",
    "        out2 = out2.squeeze()\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    '''\n",
    "    Discriminates between real data and fake data of DAYS + 1 days\n",
    "    '''\n",
    "    def __init__(self, num_features, c=5):\n",
    "        # c denotes flatenning constant\n",
    "        super().__init__()\n",
    "        self.c = c\n",
    "        self.num_features = num_features\n",
    "        self.convs = nn.Sequential(\n",
    "            nn.Conv1d(self.num_features, 32, kernel_size=5, stride=2),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Conv1d(32, 64, kernel_size=5, stride=2),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.BatchNorm1d(64, momentum=0.9, eps=1e-05), \n",
    "            nn.Conv1d(64, 128, kernel_size=5, stride=2),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.BatchNorm1d(128, momentum=0.9, eps=1e-05),\n",
    "        )\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Linear(128 * self.c, 220),\n",
    "            nn.BatchNorm1d(220, momentum=0.9, eps=1e-05),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Linear(220, 220),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(220, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, inp):\n",
    "        inp = torch.transpose(inp, 2, 1)\n",
    "        out1 = self.convs(inp)\n",
    "        out1 = out1.reshape(-1, 128 * self.c)\n",
    "        out2 = self.dense(out1)\n",
    "        out2 = out2.squeeze()\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "generator = Generator(TOTAL_FEATURES, DAYS)\n",
    "discriminator = Discriminator(TOTAL_FEATURES)\n",
    "    \n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=0.002)\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=0.002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:15<00:00,  3.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: g_loss=113.8384, d_loss=35.4277, mae=51087.7522\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:08<00:00,  5.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2: g_loss=259.8348, d_loss=9.2406, mae=51087.0350\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:08<00:00,  5.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3: g_loss=287.1808, d_loss=12.0325, mae=51087.0350\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:08<00:00,  5.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4: g_loss=250.8051, d_loss=7.8515, mae=51087.0350\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:08<00:00,  5.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5: g_loss=302.1500, d_loss=7.9688, mae=51087.0350\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:08<00:00,  5.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6: g_loss=234.4207, d_loss=7.7787, mae=51087.0350\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:08<00:00,  6.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7: g_loss=360.6979, d_loss=2.2033, mae=51087.0350\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:07<00:00,  6.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8: g_loss=277.1443, d_loss=2.7217, mae=51087.0350\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:07<00:00,  6.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9: g_loss=389.9433, d_loss=5.1676, mae=51087.0350\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:07<00:00,  6.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 10: g_loss=294.4085, d_loss=3.9021, mae=51087.0350\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:08<00:00,  5.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 11: g_loss=408.3199, d_loss=0.9840, mae=51087.0350\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:07<00:00,  6.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 12: g_loss=419.9713, d_loss=0.1993, mae=51087.0350\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:07<00:00,  6.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 13: g_loss=438.6294, d_loss=0.0953, mae=51087.0350\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:07<00:00,  6.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 14: g_loss=456.5538, d_loss=0.0653, mae=51087.0350\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:07<00:00,  6.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 15: g_loss=469.9033, d_loss=0.0506, mae=51087.0350\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:09<00:00,  5.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 16: g_loss=480.9430, d_loss=0.0411, mae=51087.0350\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:09<00:00,  5.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 17: g_loss=490.4641, d_loss=0.0344, mae=51087.0350\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:08<00:00,  5.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 18: g_loss=498.8964, d_loss=0.0294, mae=51087.0350\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:08<00:00,  5.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 19: g_loss=506.5244, d_loss=0.0256, mae=51087.0350\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:08<00:00,  5.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 20: g_loss=513.5109, d_loss=0.0225, mae=51087.0350\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:07<00:00,  6.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 21: g_loss=519.9827, d_loss=0.0199, mae=51087.0350\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48/48 [00:09<00:00,  5.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 22: g_loss=526.0050, d_loss=0.0178, mae=51087.0350\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 21/48 [00:04<00:05,  5.04it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-223-515c48f3d95a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m# loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0merror_G\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0merror_G\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;31m# step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0moptimizer_G\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    running_G_loss, running_D_loss = 0, 0\n",
    "    mae, cnt = 0, 0\n",
    "    for data in tqdm(train_dataloader):\n",
    "        #### discriminator\n",
    "        discriminator.zero_grad()\n",
    "        real_prev, real_now = data[:, :-1], data[:, -1]\n",
    "        labels = torch.ones(len(data)).float()\n",
    "        \n",
    "        ## real\n",
    "        # forward\n",
    "        output = discriminator(data)\n",
    "        # loss\n",
    "        error_D_real = criterion(output, labels)\n",
    "        error_D_real.backward()\n",
    "\n",
    "        ## fake\n",
    "        labels.fill_(0)\n",
    "        # forward on generator\n",
    "        fake_now = generator(real_prev)\n",
    "        fake = torch.hstack((real_prev, fake_now.unsqueeze(1)))\n",
    "        # forward on discriminator\n",
    "        output = discriminator(fake.detach())\n",
    "        # loss\n",
    "        error_D_fake = criterion(output, labels)\n",
    "        error_D_fake.backward()\n",
    "        \n",
    "        # step\n",
    "        optimizer_D.step()\n",
    "\n",
    "        #### generator\n",
    "        labels.fill_(1)\n",
    "        optimizer_G.zero_grad()\n",
    "        # calculate discriminator output\n",
    "        output = discriminator(fake)\n",
    "        # loss\n",
    "        error_G = criterion(output, labels)\n",
    "        error_G.backward()\n",
    "        # step\n",
    "        optimizer_G.step()\n",
    "        \n",
    "        error_D = error_D_real + error_D_fake\n",
    "        running_D_loss += error_D.item()\n",
    "        running_G_loss += error_G.item()\n",
    "        mae += abs(real_now[:, 0] - fake_now[:, 0]).sum().item()\n",
    "        cnt += 1\n",
    "\n",
    "    mae /= cnt        \n",
    "    print(f'epoch {epoch + 1}: g_loss={running_G_loss:.4f}, d_loss={running_D_loss:.4f}, mae={mae:.4f}')        \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
