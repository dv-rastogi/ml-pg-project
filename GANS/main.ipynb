{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch.utils.data as Data\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.decomposition import PCA\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_FEATURES = 6\n",
    "DAYS = 60\n",
    "BATCH_SIZE = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockMarketDataReg(Data.Dataset):\n",
    "    def __init__(self, train=None, split=0.2, target_market=\"S&P\", days=60):\n",
    "        self.days = days + 1 # as last entry is to be predicted\n",
    "\n",
    "        df = pd.read_csv(f\"../data/Processed_{target_market}.csv\")\n",
    "        df['Date'] = pd.to_datetime(df['Date'], format='%Y-%m-%d')\n",
    "        df = df.sort_values(by='Date')\n",
    "        df = df.drop(columns=['Name', 'Date'])\n",
    "        df = df.fillna(0)\n",
    "        \n",
    "        # outlier detection\n",
    "        for col in df.columns:\n",
    "            q1, q3 = df[col].quantile([0.25, 0.75])\n",
    "            iqr = q3 - q1\n",
    "            lb, rb = q1 - 1.5 * iqr, q3 + 1.5 * iqr\n",
    "            for i in range(len(df)):\n",
    "                if df[col][i] > rb:\n",
    "                    df[col][i] = rb\n",
    "                if df[col][i] < lb:\n",
    "                    df[col][i] = lb\n",
    "\n",
    "        self.feature_list = list(set(df.columns) - set(['Close']))\n",
    "        if train is None:\n",
    "            u = df.iloc[0: int(len(df) * (1 - split))]\n",
    "            # pca\n",
    "            self.pca = PCA(TOTAL_FEATURES - 1)\n",
    "            self.pca_data = self.pca.fit_transform(u[self.feature_list])\n",
    "            self.closing_prices = u['Close'].to_numpy()\n",
    "            self.data = np.hstack(( self.closing_prices.reshape((-1, 1)), self.pca_data ))\n",
    "        else:\n",
    "            u = df.iloc[int(len(df) * (1 - split)) - self.days:]\n",
    "            # pca\n",
    "            self.pca_data = train.pca.transform(u[self.feature_list])\n",
    "            self.closing_prices = u['Close'].to_numpy()\n",
    "            self.data = np.hstack(( self.closing_prices.reshape((-1, 1)), self.pca_data ))\n",
    "\n",
    "        self.num_rows = u.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        end_idx = self.days + idx - 1\n",
    "        market_tens = torch.from_numpy(self.data[idx: end_idx + 1])\n",
    "        return market_tens.float()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_rows - self.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = StockMarketDataReg(target_market='S&P', split=0.2, days=DAYS)\n",
    "test_ds = StockMarketDataReg(target_market='S&P', split=0.2, days=DAYS, train=train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = Data.DataLoader(train_ds, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 61, 6])"
      ]
     },
     "execution_count": 262,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = next(iter(train_dataloader))\n",
    "sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    '''\n",
    "    Generates fake data using features of past DAYS days\n",
    "    '''\n",
    "    def __init__(self, num_features, days, hidden_size=300):\n",
    "        super().__init__()\n",
    "        self.num_features = num_features\n",
    "        self.days = days\n",
    "        self.hidden_size = hidden_size\n",
    "        # model\n",
    "        self.lstm = nn.LSTM(input_size=self.num_features, hidden_size=self.hidden_size, num_layers=1, batch_first=True)\n",
    "        self.decoder = nn.Sequential(\n",
    "                nn.Linear(self.hidden_size * self.days, self.num_features),\n",
    "                nn.Tanh()\n",
    "            )\n",
    "    def forward(self, inp):\n",
    "        out1, _ = self.lstm(inp)\n",
    "        out1 = out1.reshape((-1, self.hidden_size * self.days))\n",
    "        out2 = self.decoder(out1)\n",
    "        out2 = out2.squeeze()\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    '''\n",
    "    Discriminates between real data and fake data of DAYS + 1 days\n",
    "    '''\n",
    "    def __init__(self, num_features, c=5):\n",
    "        # c denotes flatenning constant\n",
    "        super().__init__()\n",
    "        self.c = c\n",
    "        self.num_features = num_features\n",
    "        self.convs = nn.Sequential(\n",
    "            nn.Conv1d(self.num_features, 32, kernel_size=5, stride=2),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Conv1d(32, 64, kernel_size=5, stride=2),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.BatchNorm1d(64, momentum=0.9, eps=1e-05), \n",
    "            nn.Conv1d(64, 128, kernel_size=5, stride=2),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.BatchNorm1d(128, momentum=0.9, eps=1e-05),\n",
    "        )\n",
    "        self.dense = nn.Sequential(\n",
    "            nn.Linear(128 * self.c, 220),\n",
    "            nn.BatchNorm1d(220, momentum=0.9, eps=1e-05),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Linear(220, 220),\n",
    "            nn.LeakyReLU(0.01),\n",
    "            nn.Linear(220, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, inp):\n",
    "        inp = torch.transpose(inp, 2, 1)\n",
    "        out1 = self.convs(inp)\n",
    "        out1 = out1.reshape(-1, 128 * self.c)\n",
    "        out2 = self.dense(out1)\n",
    "        out2 = out2.squeeze()\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "generator = Generator(TOTAL_FEATURES, DAYS)\n",
    "discriminator = Discriminator(TOTAL_FEATURES)\n",
    "    \n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=0.001)\n",
    "optimizer_D = torch.optim.SGD(discriminator.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:20<00:00,  1.72s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: g_loss=8.7490, d_loss=16.6505, mae=1608.8606\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:20<00:00,  1.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2: g_loss=8.7342, d_loss=16.6267, mae=1608.9316\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:19<00:00,  1.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3: g_loss=8.7143, d_loss=16.5887, mae=1608.9316\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:18<00:00,  1.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4: g_loss=8.6937, d_loss=16.5652, mae=1608.9316\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:19<00:00,  1.63s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5: g_loss=8.6778, d_loss=16.5474, mae=1608.9316\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:12<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6: g_loss=8.6696, d_loss=16.5189, mae=1608.9316\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:10<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7: g_loss=8.6641, d_loss=16.5000, mae=1608.9316\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:12<00:00,  1.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8: g_loss=8.6524, d_loss=16.4800, mae=1608.9316\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [00:12<00:00,  1.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 9: g_loss=8.6423, d_loss=16.4615, mae=1608.9316\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 5/12 [00:04<00:06,  1.08it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-267-51c64bb9cea5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m# loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0merror_G\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0merror_G\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;31m# step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0moptimizer_G\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    running_G_loss, running_D_loss = 0, 0\n",
    "    mae, cnt = 0, 0\n",
    "    for data in tqdm(train_dataloader):\n",
    "        #### discriminator\n",
    "        discriminator.zero_grad()\n",
    "        real_prev, real_now = data[:, :-1], data[:, -1]\n",
    "        labels = torch.ones(len(data)).float()\n",
    "        \n",
    "        ## real\n",
    "        # forward\n",
    "        output = discriminator(data)\n",
    "        # loss\n",
    "        error_D_real = criterion(output, labels)\n",
    "        error_D_real.backward()\n",
    "\n",
    "        ## fake\n",
    "        labels.fill_(0)\n",
    "        # forward on generator\n",
    "        fake_now = generator(real_prev)\n",
    "        fake = torch.hstack((real_prev, fake_now.unsqueeze(1)))\n",
    "        # forward on discriminator\n",
    "        output = discriminator(fake.detach())\n",
    "        # loss\n",
    "        error_D_fake = criterion(output, labels)\n",
    "        error_D_fake.backward()\n",
    "        \n",
    "        # step\n",
    "        optimizer_D.step()\n",
    "\n",
    "        #### generator\n",
    "        labels.fill_(1)\n",
    "        optimizer_G.zero_grad()\n",
    "        # calculate discriminator output\n",
    "        output = discriminator(fake)\n",
    "        # loss\n",
    "        error_G = criterion(output, labels)\n",
    "        error_G.backward()\n",
    "        # step\n",
    "        optimizer_G.step()\n",
    "        \n",
    "        error_D = error_D_real + error_D_fake\n",
    "        running_D_loss += error_D.item()\n",
    "        running_G_loss += error_G.item()\n",
    "        mae += abs(real_now[:, 0] - fake_now[:, 0]).sum().item()\n",
    "        cnt += len(data)\n",
    "\n",
    "    mae /= cnt        \n",
    "    print(f'epoch {epoch + 1}: g_loss={running_G_loss:.4f}, d_loss={running_D_loss:.4f}, mae={mae:.4f}')        \n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
